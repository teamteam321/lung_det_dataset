{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 849\n",
      "/notebooks/Test_Env/STAT_PRED/1ch/12x12/LIDC-IDRI-0001_5_1_6_3.5_3.5_3_4.5_4.75_4.75.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "path = glob.glob(\"/notebooks/Test_Env/STAT_PRED/1ch/12x12/*.png\")\n",
    "print(\"total: \"+str(len(path)))\n",
    "\n",
    "path.sort()\n",
    "print(path[0])\n",
    "\n",
    "temp = []\n",
    "label_temp = []\n",
    "\n",
    "for ddd in range(0,9):\n",
    "    label_temp_inner = []\n",
    "    label_temp.append(label_temp_inner)\n",
    "\n",
    "for i,n in enumerate(path):\n",
    "    #temp.append(cv2.imread(n))\n",
    "    pict_t = cv2.imread(n,0)\n",
    "    label_temp_inner = []\n",
    "    \n",
    "    gray = np.ndarray((pict_t.shape[0],pict_t.shape[1],1),dtype = int)\n",
    "    gray = pict_t.reshape((pict_t.shape[0],pict_t.shape[1],1))\n",
    "    \n",
    "    temp.append(gray)\n",
    "    noname = n[n.rindex(\"/\"):]\n",
    "    noname = noname[noname.index(\"_\")+1:]\n",
    "    \n",
    "    noname = noname[:noname.rindex(\".png\")]\n",
    "    k = noname.split(\"_\")\n",
    "    \n",
    "   \n",
    "    for idx in range(0,9):\n",
    "        r = np.ndarray((1,))\n",
    "        r[0] = float(k[idx])\n",
    "        label_temp[idx].append(r)\n",
    "pic = np.array(temp)\n",
    "label = np.array(label_temp)\n",
    "      \n",
    "maxlist=[]\n",
    "for i in label:\n",
    "    maxlist.append(np.max(i))\n",
    "\n",
    "for i in range (len(label)):\n",
    "    label[i]/=maxlist[i]\n",
    "    \n",
    "pic = pic / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "def get_model_x(input_shape):\n",
    "    \n",
    "    \n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    X = Conv2D(filters=32, kernel_size=(5, 5),\n",
    "               activation='relu', padding='same')(inputs)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "\n",
    "    X = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "               activation='relu', padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "\n",
    "    X = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "               activation='relu', padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2))(X)\n",
    "    \n",
    "    X = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "               activation='relu', padding='same')(X)\n",
    "\n",
    "    X = Flatten()(X)\n",
    "\n",
    "    X = Dense(1024, activation='sigmoid')(X)\n",
    "    X = Dense(512, activation='sigmoid')(X)\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    #,kernel_initializer=\"he_normal\"\n",
    "    model = Model(inputs=[inputs], outputs=[X])\n",
    "    return model\n",
    "\n",
    "def get_adaptive_lr_callback():\n",
    "    from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "    def lr_scheduler(epoch, lr):\n",
    "        decay_rate = 0.1\n",
    "        decay_step = 50\n",
    "        if epoch % decay_step == 0 and epoch > decay_step:\n",
    "            return lr * decay_rate\n",
    "        return lr\n",
    "\n",
    "    adaptive_lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "    return adaptive_lr_callback\n",
    "\n",
    "def compile_model(model, lr):\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "    from tensorflow import keras\n",
    "    optimizer = Adam(lr=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=keras.losses.mean_squared_error,\n",
    "                  metrics=[\n",
    "                      keras.metrics.mean_absolute_error,\n",
    "                      keras.metrics.mean_squared_error\n",
    "                  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save evaluation txt\n",
    "\n",
    "def save_eval(i,h,model,name = \"temp.txt\"):\n",
    "    fs = open('./'+name,'a')\n",
    "    fs.write(str(i+1)+\" \"+str(min(h.history['val_mean_absolute_error'])*maxlist[i])+'\\n')\n",
    "    print('finished: '+str(i+1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save model \n",
    "import gc\n",
    "save_path = './final_model2'\n",
    "import pickle\n",
    "# 5.0, 4.0, 6.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0\n",
    "namedict=['place_holder',\n",
    "        'Subtlety',\n",
    "        'Internal Structure',\n",
    "         'Calcification',\n",
    "         'Sphericity',\n",
    "         'Margin',\n",
    "         'Lobulation',\n",
    "         'Spiculation',\n",
    "         'Texture',\n",
    "         'Malignancy']\n",
    "\n",
    "for i in range(0,9):\n",
    "    \n",
    "    mcp_save = keras.callbacks.ModelCheckpoint(save_path+'/'+namedict[i+1]+'_1ch_12x12.hdf5', save_best_only=True,verbose = 0, monitor='val_mean_absolute_error', mode='min')\n",
    "    \n",
    "    \n",
    "    #print(pic.shape)\n",
    "    split=int(len(pic)*0.80)\n",
    "    print(\"start: \"+str(i+1))\n",
    "    h_plot = []\n",
    "        \n",
    "    model = get_model_x((128,128,1))\n",
    "    model = compile_model(model,5e-5)\n",
    "    \n",
    "    h=model.fit(pic,label[i],validation_split = 0.2 ,batch_size=32,epochs=1,verbose=0,callbacks=[\n",
    "            get_adaptive_lr_callback(),\n",
    "            mcp_save\n",
    "    ])\n",
    "    save_eval(i,h,model,'all_train.txt')\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtlety\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# train and test by fold\n",
    "namedict=['Subtlety',\n",
    "        'Internal Structure',\n",
    "         'Calcification',\n",
    "         'Sphericity',\n",
    "         'Margin',\n",
    "         'Lobulation',\n",
    "         'Spiculation',\n",
    "         'Texture',\n",
    "         'Malignancy']\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(len(pic))\n",
    "\n",
    "result = {}\n",
    "for i in range(0,1):\n",
    "    #9 stats\n",
    "    print(namedict[i])\n",
    "    result[namedict[i]] = {}\n",
    "    for fold_id, (train_index, test_index) in enumerate(kf.split(pic), start=1):\n",
    "        # by fold\n",
    "        \n",
    "        print(fold_id)\n",
    "        #print(train_index[0],train_index[-1], test_index[0],test_index[-1] )\n",
    "        model = get_model_x((128,128,1))\n",
    "        model = compile_model(model,5e-5)\n",
    "    \n",
    "        h=model.fit(pic[train_index],label[i][train_index],\n",
    "                    validation_data=(pic[test_index],label[i][test_index]),\n",
    "                    batch_size=30,\n",
    "                    epochs=150,\n",
    "                    verbose=0,\n",
    "                    callbacks=[\n",
    "            get_adaptive_lr_callback()\n",
    "        ])\n",
    "        result[namedict[i]]['fold'+str(fold_id)] = min(h.history['val_mean_absolute_error'])*maxlist[i]\n",
    "        del model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result_fold.json','w') as jjj:\n",
    "    json.dump(result,jjj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
